{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nYjSFp6PtcR"
      },
      "source": [
        "# Assignment 1: K-Nearest Neighbor (10 marks)\n",
        "\n",
        "Student Name: -\n",
        "\n",
        "Student ID: -\n",
        "\n",
        "## General info\n",
        "\n",
        "<b>Due date</b>: Friday, 18 March 2022 5pm\n",
        "\n",
        "<b>Submission method</b>: Canvas submission\n",
        "\n",
        "<b>Submission materials</b>: completed copy of this iPython notebook\n",
        "\n",
        "<b>Late submissions</b>: -10% per day up to 5 days (both weekdays and weekends count). Submissions more than 5 days late will not be accepted (resul in a mark of 0).\n",
        "<ul>\n",
        "    <li>one day late, -1.0;</li>\n",
        "    <li>two days late, -2.0;</li>\n",
        "    <li>three days late, -3.0;</li>\n",
        "    <li>four days late, -4.0;</li>\n",
        "    <li>five days late, -5.0;</li>\n",
        "</ul>\n",
        "\n",
        "<b>Extensions</b>: Students who are demonstrably unable to submit a full solution in time due to medical reasons or other trauma, may apply for an extension.  In these cases, you should email <a href=\"mailto:hasti.samadi@unimelb.edu.au\">Hasti Samadi</a> as soon as possible after those circumstances arise. If you attend a GP or other health care service as a result of illness, be sure to provide a Health Professional Report (HPR) form (get it from the Special Consideration section of the Student Portal), you will need this form to be filled out if your illness develops into something that later requires a Special Consideration application to be lodged. You should scan the HPR form and send it with the extension requests.\n",
        "\n",
        "<b>Marks</b>: This assignment will be marked out of 10, and make up 10% of your overall mark for this subject.\n",
        "\n",
        "<b>Materials</b>: See [Using Jupyter Notebook and Python page](https://canvas.lms.unimelb.edu.au/courses/124196/pages/python-and-jupyter-notebooks?module_item_id=3512182) on Canvas (under Modules> Coding Resources) for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Scikit-Learn. You can use any Python built-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>.  \n",
        "\n",
        "\n",
        "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should edit the sections below where requested, but leave the rest of the code as is. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions.\n",
        "\n",
        "You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it. We reserve the right to deduct up to 2 marks for unreadable or exessively inefficient code.\n",
        "\n",
        "<b>Updates</b>: Any major changes to the assignment will be announced via Canvas. Minor changes and clarifications will be announced on the discussion board (Piazza -> Assignments -> A1); we recommend you check it regularly.\n",
        "\n",
        "<b>Academic misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. Please check the <a href=\"https://canvas.lms.unimelb.edu.au/courses/124196/modules#module_662096\">CIS Academic Honesty training</a> for more information. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n",
        "\n",
        "**IMPORTANT**\n",
        "\n",
        "Please carefully read and fill out the <b>Authorship Declaration</b> form at the bottom of the page. Failure to fill out this form results in the following deductions:\n",
        "<UL TYPE=”square”>\n",
        "<LI>missing Authorship Declaration at the bottom of the page, -5.0\n",
        "<LI>incomplete or unsigned Authorship Declaration at the bottom of the page, -3.0\n",
        "</UL>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8y2oCPgPtcV"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this homework, you'll be applying the K-nearest neighbor (KNN) classification algorithm to a real-world machine learning data set. In particular, we will predict the primary color of national flags given a diverse set of features, including the country's size and population and other structural properties of the flag.\n",
        "\n",
        "Firstly, you will read in the dataset into a train and a test set, and you will create two feature sets (Q1). Secondly, you will implement different distance functions (Q2). Thirdly, you will implement two KNN classifiers (Q3, Q4) and apply it to the data set using different distance functions and parameter K (Q5). Finally, you will assess the quality of your classifier by comparing its class predictions to the true (or \"gold standard\") labels (Q6).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AokCHcglPtcW"
      },
      "source": [
        "### Question 1: Loading the data (1.0 marks)\n",
        "\n",
        "**Instructions:** For this assignment we will develop a K-Nearest Neighbors (KNN) classifier to predict the predominant color of national flags. The list of classes (colors) is:\n",
        "\n",
        "```\n",
        "black\n",
        "blue\n",
        "brown\n",
        "gold\n",
        "green\n",
        "orange\n",
        "red\n",
        "white\n",
        "```\n",
        "\n",
        "We use a modified version of the Flags data set from the UCI Machine learning repository.\n",
        "\n",
        "The original data can be found here: https://archive.ics.uci.edu/ml/datasets/Flags.\n",
        "\n",
        "The dataset consists of 194 instances. Each instance corresponds to a national flag which has a unique identifier (itemX; first field) and is characterized with 25 features as described in the file *flags.names* which is provided as part of this assignment.\n",
        "\n",
        "You need to first obtain this dataset, which is on Canvas (assignment 1). The files *flags.features* and *flags.labels* contain the data we will use in this notebook. Make sure the files are saved in the same folder as this notebook.\n",
        "\n",
        "Both files are in comma-separated value (csv) format. The first line in each file is a header, naming each feature (or label).\n",
        "\n",
        "*flags.features* contains 194 instances, one line per instance. The first field is the unique instance identifier (name of country). The following fields contain the 25 features, as described in the file *flags.names*.\n",
        "\n",
        "*flags.labels* contains the true labels (i.e., one of the nine color classes above), one instance per line. Again, the first field is the instance identifier, and the second field the instance label.\n",
        "\n",
        "*flags.names* contains additional explanations about the data set and the features.\n",
        "\n",
        "All feature values are integers, and for Questions 1 through 5, we make the simplifying assumption that all values are indeed numeric. You may want to revisit this assumption in Question 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOZD7qzxPtcW"
      },
      "source": [
        "### Question 1a [0.5 mark]\n",
        "\n",
        "**Task**: Read the two files  \n",
        "1. create a **training_feature** set (list of features for the first 150 instances in the flags.* files) and a **training_label** set (list of labels for the corresponding).\n",
        "2. create a **test_feature** set (list of features of the remaining instances in the flags.* files) and a **test_label** set (list of labels for the corresponding).\n",
        "---------\n",
        "- Do **not** shuffle the data.\n",
        "- Do **not** modify feature or label representations.\n",
        "--------\n",
        "You may use any Python packages you want, but not change the specified data types (i.e., they should be of type List, and *not* dataframe, dictionary etc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyUspDT8PtcX"
      },
      "outputs": [],
      "source": [
        "data = open(\"flags.features\", 'r').readlines()\n",
        "labels = open(\"flags.labels\", 'r').readlines()\n",
        "\n",
        "\n",
        "train_features = []\n",
        "train_labels   = []\n",
        "test_features = []\n",
        "test_labels   = []\n",
        "\n",
        "\n",
        "###########################\n",
        "## YOUR CODE BEGINS HERE\n",
        "###########################\n",
        "\n",
        "header = data[0]\n",
        "instances = data[1:]\n",
        "\n",
        "real_labels = labels[1:] # getting all rows of labels except header [0]\n",
        "\n",
        "\n",
        "for i in range(len(instances)):\n",
        "    instance = instances[i] # getting each instance of data\n",
        "    instance = instance.strip() # strip the string\n",
        "    instance = instance.split(',') # splite by , and make our lists\n",
        "    label = real_labels[i] # since we have instance and number of rows are equal and itemX names in the first column is sorted in both datasets we can access them by same index number\n",
        "    label = label.strip() # same as instance\n",
        "    label = label.split(',') # same as istance\n",
        "    inst_features = instance[1:] # getting all features in each instance except the first one which will be our label\n",
        "\n",
        "    if i<150: # getting the first 150 instances as training features and labels\n",
        "        train_features.append(inst_features)\n",
        "        train_labels.append(label[1])# mainhue instead of the name\n",
        "    elif i>=150: # getting the rest of data as test features and labels\n",
        "        test_features.append(inst_features)\n",
        "        test_labels.append(label[1])\n",
        "\n",
        "###########################\n",
        "## YOUR CODE ENDS HERE\n",
        "###########################\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qAt0kRkPtcY"
      },
      "source": [
        "### Question 1b [0.5 marks]\n",
        "\n",
        "**Task** Create a reduced feature set which only includes the \"Structural Flag Features\". The file *flag.names* specifies these features.\n",
        "\n",
        "----------\n",
        "\n",
        "You may use any Python packages you want, but not change the specified data types. You may (but don't have to) hard-code feature indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1T1kRVvcPtcZ"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "## YOUR CODE BEGINS HERE\n",
        "###########################\n",
        "\n",
        "test_features_structure = [[ins[i] for i in range(len(ins)) if i>4] for ins in test_features]\n",
        "train_features_structure = [[ins[i] for i in range(len(ins)) if i>4] for ins in train_features]\n",
        "\n",
        "###########################\n",
        "## YOUR CODE ENDS HERE\n",
        "###########################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SloFoGECPtcZ"
      },
      "source": [
        "### Question 2: Distance Functions [1.0 mark]\n",
        "\n",
        "<b>Instructions</b>: Implement the two distance functions specified below.\n",
        "\n",
        "1. Manhattan distance\n",
        "2. Cosine distance\n",
        "\n",
        "Each distance function takes as input\n",
        "- Two feature vectors (each of type List)\n",
        "\n",
        "and returns as output\n",
        "- The distance between the two feature vectors (float)\n",
        "\n",
        "------------\n",
        "\n",
        "Use <b>only</b> the library imported below, i.e., <b>do not</b> use implementations from any other Python library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Hu4yzuUPtca"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "def manhattan_distance(fw1, fw2):\n",
        "    # insert code here\n",
        "    distance = 0.0\n",
        "    for i in range(len(fw1)):\n",
        "        distance = distance + abs(fw1[i]-fw2[i])\n",
        "    return distance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def cosine_distance(fw1, fw2):\n",
        "    # insert code here\n",
        "    up = 0.0\n",
        "    down = 0.0\n",
        "    fw1Sum = 0.0\n",
        "    fw2Sum = 0.0\n",
        "    for i in range(len(fw1)):\n",
        "        up = up + float(fw1[i]*fw2[i])\n",
        "        fw1Sum = fw1Sum + fw1[i]**2\n",
        "        fw2Sum = fw2Sum + fw2[i]**2\n",
        "    down = math.sqrt(fw1Sum) * math.sqrt(fw2Sum)\n",
        "    distance = up/down\n",
        "    return distance\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFY-ysguPtca"
      },
      "source": [
        "### Question 3: KNN Classifier [2.0 marks]\n",
        "\n",
        "<b>Instructions</b>: Here, you implement your KNN classifier. It takes as input\n",
        "- training data features\n",
        "- training data labels\n",
        "- test data features\n",
        "- parameter K\n",
        "- distance function(s) based on which nearest neighbors will be identified\n",
        "\n",
        "It returns as output\n",
        "- the predicted labels for the test data\n",
        "\n",
        "**Ties among distances**. If there are more than K instances with the same (smallest) distance value, consider the first K.\n",
        "\n",
        "**Ties at prediction time.** Ties can also occur at class prediction time when two (or more) classes are supported by the same number of neighbors. In that case choose the class of the 1 nearest neighbor.\n",
        "\n",
        "-----------\n",
        "\n",
        "**You should implement the classifier from scratch yourself**, i.e., <b> you must not</b> use an existing implementation in any Python library. You may use Python packages (e.g., math, numpy, collections, ...) to help with your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecCGTFG0Ptca"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "\n",
        "def KNN(train_features, train_labels, test_features, k, dist_fun, weighted=False):\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    ###########################\n",
        "    ## Your answer BEGINS HERE\n",
        "    ###########################\n",
        "\n",
        "    for i in test_features:\n",
        "        distances = []\n",
        "        predicted_labels = []\n",
        "        i_integer_map = map(int, i)\n",
        "        i_integer_list = list(i_integer_map)\n",
        "        # getting the distances by appropriate func:\n",
        "        for j in train_features:\n",
        "            j_integer_map = map(int, j)\n",
        "            j_integer_list = list(j_integer_map)\n",
        "            distances.append(dist_fun(i_integer_list,j_integer_list))\n",
        "        #sorting all distances using np and keeping the first K in 'indices'\n",
        "        indices = np.argsort(np.array(distances))[:k]#if we have multiple equal distances we shall get the first nearest one while Tie 2 will be checked.\n",
        "        #print(indices) e.g [ 13 113  39 144]\n",
        "        for knearest in indices:\n",
        "            predicted_labels.append(train_labels[knearest])\n",
        "\n",
        "\n",
        "        prepre = '' # this will be used with 'temppred' for applying ties with same class name(labels) before appendinding the right classes to 'predictions' list(nearest Neighbor among same classes)\n",
        "        if weighted==False:\n",
        "            if k%2!=0: # if k is odd then there wouldnt be a tie on nearest labels\n",
        "                prepre=Counter(predicted_labels).most_common(k)[0][0] # in this case we will get the lable base on Majority class - k is odd so we have majority vote\n",
        "            else:\n",
        "                aa = Counter(predicted_labels).most_common(k) # aa is the most commmon k neighbors' labels\n",
        "                countsOfLabelDict = defaultdict(int) # i want to count base on the counts of each label for example: blue:3,red:3 will be: 3:2 meaning 3 apeared 2 times which can make a tie in our set\n",
        "                tempansw = 0 # a flag var to help us check if we have a tie with the highest number of repeats\n",
        "                temppred = [] #\n",
        "                for ii in aa: # for each label counter value ill get its number of time that it has been appeared in near test data\n",
        "                    countsOfLabelDict[ii[1]] = countsOfLabelDict[ii[1]] + 1 # ii[1] is the number of time 'blue' apeared for example\n",
        "                for kk in countsOfLabelDict: # after making a dict of numbers of apearances we check for ties( check if same classes are there with maximum repeats)\n",
        "                    if countsOfLabelDict[kk] == k:# this means there is ONE of each class and since its equal to the size of K (NN) it means we have a tie : one of each class... we get the closest one to the test data\n",
        "                        prepre=train_labels[0]\n",
        "                    elif countsOfLabelDict[kk]>1:# if its more that 1 it means there are more than 1 classes with the samen number of repeatation e.g blue:3 red:3 => {3:2} 3 apeared twice\n",
        "                        #print(\"tie?\")\n",
        "                        for n in aa: # we shall check in k nearest to test to see if the first class repeats is equal to the 'kk' we have recently in our 'countsOfLabelDict'\n",
        "                            if n[1] == kk:\n",
        "                                if n[1] < aa[0][1] and n[1] != 1:# if its smaller than the first one and its not apeared only once then its not a tie e.g blue:3 red:2 green:1 => 3:1,2:1,1:1 since k is even its 1 if is not working\n",
        "                                    #print(\"no tie\")\n",
        "                                    tempansw = 0 #since it might be changed in next if statement we want to make sure this flag will be 0 to show no tie\n",
        "\n",
        "\n",
        "                                elif n[1] == aa[0][1]:\n",
        "                                    tempansw = tempansw + 1\n",
        "                                    temppred.append(n[0]) # not only increase tempansw to show that we have seen nearest in counters 'n' we wanna save this tie label so we know what labels had tie eventually\n",
        "\n",
        "\n",
        "                    #elif countsOfLabelDict[kk]==1:\n",
        "                        #print(\"all are majority class\")\n",
        "                        #continue\n",
        "\n",
        "                #now its time to show ties and evaluate the 'predictions'\n",
        "                flagg = 0\n",
        "                if tempansw>1:# a label apeared more than onece : once it sees itsef atleast since the list is sorted\n",
        "                    #print(\"tie\")\n",
        "                    for tie2 in temppred:\n",
        "                        for knearest in indices:\n",
        "                            if train_labels[knearest]==tie2:\n",
        "                                if flagg == 0:\n",
        "                                    tempdis = distances[knearest]\n",
        "                                    prepre = tie2\n",
        "                                    flagg = 1\n",
        "                                    # we check if the tie we recorded is actually the lowest distance or not so we use a flagg to check if its the minimum or else we replace it and the label for further prediction appending\n",
        "                                else:\n",
        "                                    if tempdis > distances[knearest]:\n",
        "                                        tempdis = distances[knearest]\n",
        "                                        prepre = tie2\n",
        "                    #print(prepre)\n",
        "                else:\n",
        "                    #print(\"no tie\")\n",
        "                    prepre = aa[0][0]\n",
        "                #print(tempansw)\n",
        "\n",
        "            predictions.append(prepre) # ==============> appending the prediction for this test instance\n",
        "            #print()\n",
        "        #-----------------------------\n",
        "        else:# weighted is TRUE\n",
        "            e = 0.00001\n",
        "            weights = []\n",
        "            weightedDisLabel = defaultdict(float)\n",
        "            for distance in indices:\n",
        "                #calculating the distance for k nearest neighbors and appending it to a list\n",
        "                dom = distances[distance] + e\n",
        "                w = 1/dom\n",
        "                weights.append(w)\n",
        "\n",
        "            for l in range(len(predicted_labels)):\n",
        "\n",
        "                weightedDisLabel[predicted_labels[l]] = weightedDisLabel[predicted_labels[l]] + weights[l]\n",
        "            #print(weightedDisLabel)\n",
        "            biggestWeight = sorted(weightedDisLabel.items(), key=lambda item: item[1],reverse=True)\n",
        "            predictions.append(biggestWeight[0][0]) # ==============> appending the prediction for this test instance\n",
        "\n",
        "\n",
        "    ###########################\n",
        "    ## Your answer ENDS HERE\n",
        "    ###########################\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6WXevOUPtcb"
      },
      "source": [
        "### Question 4: Weighted KNN Classifier [1.0 mark]\n",
        "\n",
        "<b>Instructions</b>: Extend your implementation of the KNN classifier in Question 3 to a Weighted KNN classifier. Use Inverse Distance as weights:\n",
        "\n",
        "$w_j=\\frac{1}{d_j+\\epsilon}$\n",
        "\n",
        "where\n",
        "\n",
        "- $d_j$ is the distance of of the jth nearest neighbor to the test instance\n",
        "- $\\epsilon=0.00001$\n",
        "\n",
        "Use the Boolean parameter `weighted` to specify the KNN version when calling the function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va9cr_djPtcb"
      },
      "source": [
        "### Question 5: Applying your KNN classifiers to the Flags Dataset [0.5 marks]\n",
        "\n",
        "**Using the functions you have implemented above, please**\n",
        "\n",
        "<b> 1. </b>\n",
        "For each of the distance functions you implemented in Question 2, construct (a) two majority voting KNN classifiers and (b) two weighted KNN classifiers, respectively, with\n",
        "\n",
        "- K=1\n",
        "- K=5\n",
        "\n",
        "You will obtain a total of 16 (2 distance functions x 2 K values x 2 KNN versions x 2 feature sets) classifiers.\n",
        "\n",
        "<b> 2. </b>\n",
        "Compute the test accuracy for each model, where the accuracy is the fraction of correctly predicted labels over all predictions. Use the `accuracy_score` function from the `sklearn.metrics` package to obtain your accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t24NOJfBPtcb",
        "outputId": "84d71d00-7272-4a83-9246-e911d4362d3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results on the *full* feature set\n",
            "\n",
            "manhattan (majority vote)\n",
            "K=1 0.273\n",
            "K=5 0.364\n",
            "-----------\n",
            "manhattan (weighted)\n",
            "K=1 0.273\n",
            "K=5 0.341\n",
            "\n",
            "cosine (majority vote)\n",
            "K=1 0.068\n",
            "K=5 0.159\n",
            "-----------\n",
            "cosine (weighted)\n",
            "K=1 0.068\n",
            "K=5 0.159\n",
            "\n",
            "\n",
            "Results on the *structure* feature set\n",
            "\n",
            "manhattan (majority vote)\n",
            "K=1 0.295\n",
            "K=5 0.318\n",
            "-----------\n",
            "manhattan (weighted)\n",
            "K=1 0.295\n",
            "K=5 0.364\n",
            "\n",
            "cosine (majority vote)\n",
            "K=1 0.205\n",
            "K=5 0.159\n",
            "-----------\n",
            "cosine (weighted)\n",
            "K=1 0.205\n",
            "K=5 0.159\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "########################\n",
        "# Your code STARTS HERE\n",
        "########################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "accuracy_knn_man_1 = accuracy_score(test_labels, KNN(train_features, train_labels, test_features, 1, manhattan_distance))\n",
        "accuracy_knn_man_5 = accuracy_score(test_labels, KNN(train_features, train_labels, test_features, 5, manhattan_distance))\n",
        "\n",
        "accuracy_knn_man_1_structure = accuracy_score(test_labels, KNN(train_features_structure, train_labels, test_features_structure, 1, manhattan_distance))\n",
        "accuracy_knn_man_5_structure = accuracy_score(test_labels, KNN(train_features_structure, train_labels, test_features_structure, 5, manhattan_distance))\n",
        "\n",
        "accuracy_knn_man_1_w = accuracy_score(test_labels, KNN(train_features, train_labels, test_features, 1, manhattan_distance,weighted=True))\n",
        "accuracy_knn_man_5_w = accuracy_score(test_labels, KNN(train_features, train_labels, test_features, 5, manhattan_distance,weighted=True))\n",
        "\n",
        "accuracy_knn_man_1_w_structure = accuracy_score(test_labels, KNN(train_features_structure, train_labels, test_features_structure, 1, manhattan_distance,weighted=True))\n",
        "accuracy_knn_man_5_w_structure = accuracy_score(test_labels, KNN(train_features_structure, train_labels, test_features_structure, 5, manhattan_distance,weighted=True))\n",
        "\n",
        "accuracy_knn_cos_1 = accuracy_score(test_labels, KNN(train_features, train_labels, test_features, 1, cosine_distance))\n",
        "accuracy_knn_cos_5 = accuracy_score(test_labels, KNN(train_features, train_labels, test_features, 5, cosine_distance))\n",
        "\n",
        "accuracy_knn_cos_1_structure = accuracy_score(test_labels, KNN(train_features_structure, train_labels, test_features_structure, 1, cosine_distance))\n",
        "accuracy_knn_cos_5_structure = accuracy_score(test_labels, KNN(train_features_structure, train_labels, test_features_structure, 5, cosine_distance))\n",
        "\n",
        "accuracy_knn_cos_1_w = accuracy_score(test_labels, KNN(train_features, train_labels, test_features, 1, cosine_distance,weighted=True))\n",
        "accuracy_knn_cos_5_w = accuracy_score(test_labels, KNN(train_features, train_labels, test_features, 5, cosine_distance,weighted=True))\n",
        "\n",
        "accuracy_knn_cos_1_w_structure = accuracy_score(test_labels, KNN(train_features_structure, train_labels, test_features_structure, 1, cosine_distance,weighted=True))\n",
        "accuracy_knn_cos_5_w_structure = accuracy_score(test_labels, KNN(train_features_structure, train_labels, test_features_structure, 5, cosine_distance,weighted=True))\n",
        "\n",
        "\n",
        "########################\n",
        "# Your code ENDS HERE\n",
        "########################\n",
        "\n",
        "\n",
        "\n",
        "print(\"Results on the *full* feature set\")\n",
        "\n",
        "print(\"\\nmanhattan (majority vote)\")\n",
        "print(\"K=1\", round(accuracy_knn_man_1, 3))\n",
        "print(\"K=5\", round(accuracy_knn_man_5, 3))\n",
        "\n",
        "print(\"-----------\\nmanhattan (weighted)\")\n",
        "print(\"K=1\", round(accuracy_knn_man_1_w, 3))\n",
        "print(\"K=5\", round(accuracy_knn_man_5_w, 3))\n",
        "\n",
        "print(\"\\ncosine (majority vote)\")\n",
        "print(\"K=1\", round(accuracy_knn_cos_1, 3))\n",
        "print(\"K=5\", round(accuracy_knn_cos_5, 3))\n",
        "\n",
        "print(\"-----------\\ncosine (weighted)\")\n",
        "print(\"K=1\", round(accuracy_knn_cos_1_w, 3))\n",
        "print(\"K=5\", round(accuracy_knn_cos_5_w, 3))\n",
        "\n",
        "print(\"\\n\\nResults on the *structure* feature set\")\n",
        "\n",
        "print(\"\\nmanhattan (majority vote)\")\n",
        "print(\"K=1\", round(accuracy_knn_man_1_structure, 3))\n",
        "print(\"K=5\", round(accuracy_knn_man_5_structure, 3))\n",
        "\n",
        "print(\"-----------\\nmanhattan (weighted)\")\n",
        "print(\"K=1\", round(accuracy_knn_man_1_w_structure, 3))\n",
        "print(\"K=5\", round(accuracy_knn_man_5_w_structure, 3))\n",
        "\n",
        "print(\"\\ncosine (majority vote)\")\n",
        "print(\"K=1\", round(accuracy_knn_cos_1_structure, 3))\n",
        "print(\"K=5\", round(accuracy_knn_cos_5_structure, 3))\n",
        "\n",
        "print(\"-----------\\ncosine (weighted)\")\n",
        "print(\"K=1\", round(accuracy_knn_cos_1_w_structure, 3))\n",
        "print(\"K=5\", round(accuracy_knn_cos_5_w_structure, 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAWm2XqMPtcc"
      },
      "source": [
        "### Question 6: Analysis [4.5 marks]\n",
        "1. (a) Discuss the appropriateness of each of the distance functions to our two versions of the *flags* data set. Where appropriate, explain why you expect them to perform poorly referring to both their mathematical properties and the given feature set. **[0.5 marks]**\n",
        "\n",
        "    (b) Imagine you could choose a third distance function for the *Structure* feature set: either Hamming or Euclidean. Which one would you choose and why? Do you expect the results to be similar or different from Manhattan and Cosine [*N.B. you should only hypothesize based on the definitions of the metrics. You do not need to write any code*] **[0.5 marks]**\n",
        "    \n",
        "\n",
        "2. Does the Weighted KNN outperform the Majority voting version, or vice versa? Hypothesize why (not). **[1 mark]**\n",
        "\n",
        "\n",
        "3. (a) Plot a <a href=\"https://en.wikipedia.org/wiki/Histogram\">histogram</a> of the actual class frequencies in the test set, and a histogram of the predicted test labels for the knn_man_5 model. You should produce **a single plot** which shows the histogram both true and predicted labels. Label the x-axis and y-axis. [*N.B. you may use libraries like <a href=\"https://matplotlib.org/stable/tutorials/introductory/usage.html#sphx-glr-tutorials-introductory-usage-py\">matplotlib</a> or <a href=\"https://seaborn.pydata.org/introduction.html\">seaborne</a>*] **[1 mark]**\n",
        "\n",
        "    (b) Describe and explain the discrepancy between the true and predicted distributions. **[1 mark]**\n",
        "\n",
        "\n",
        "4. Do you think the accuracy is an appropriate evaluation metric for the *Flags* data set? Why (not)? **[0.5 marks]**\n",
        "\n",
        "<b>Each question should be answered in no more than 3-4 sentences.</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwzbGkclPtcd"
      },
      "source": [
        "1a) After comparing accuracies, firstly structured data can perform better generally due to fewer noises in our data. Secondly, Cosine distance can perform better with structured data since it is normalized by magnitude and noises can be a problem.\n",
        "\n",
        "1b) For the structure feature set, Eucleadian should perform better since hamming is used for Nominal feature sets and what we have in our dataset is mostly numerical here. On the other hand, since Manhattan distance can perform well on high dimensional data, I think it can have a similar outcome. considering that none of them Euclidian and Manhattan are not normalized.\n",
        "\n",
        "2) Here, I can see weighted KNN could outperform Majority. That can be because of the similarity between labels for each instance and therefore bigger sum over a similar label. The presence of epsilon makes the data scatter more in the Denominator.\n",
        "\n",
        "\n",
        "*Type code for 3.(a) in the cell below, and answer 3.(b) below*\n",
        "\n",
        "3b) Looking at the bars in the histogram, Red has the most number of labels in both sets and by looking at the set of colours (labels) in the actual test we cannot find a variety of colours and some colours are absent like 'brown', which can be due to a bad data selection. The same problem can be seen in the predicted labels. so the test data is not distributed well over labels.(discrimination)\n",
        "\n",
        "4 Although we know our data selection has not been fully covered as discussed above, I believe accuracy cannot be the best evaluation since it only compares the values in case of equality. In addition, looking at the histogram we can see the Orange bar is almost filling more than 50% of each blue bar. Since we are predicting one colour for a country flag we shall be interested in how accurately we measured that special colour, not a general 'error' or 'accuracy'. Accuracy is the opposite of Error and error is a function of K in KNN. So there might be better options like recall and precision to check for each class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "AAKBAmtEPtcd",
        "outputId": "5fac186c-7bbd-429d-f43b-bb7e7391d4e9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################\n",
        "# Your answer to Question 6 (3) STARTS HERE\n",
        "################################################\n",
        "#actual class freq in the test set and predicted test labels for knn man 5\n",
        "\n",
        "x = KNN(train_features, train_labels, test_features, 5, manhattan_distance)\n",
        "z = test_labels\n",
        "\n",
        "\n",
        "plt.hist(x, bins=10)\n",
        "plt.hist(z, bins=20)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "################################################\n",
        "# Your answer to Question 6 (3) ENDS HERE\n",
        "################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RGQcZGyPtcd"
      },
      "source": [
        "<b>Authorship Declaration</b>:\n",
        "\n",
        "   (1) I certify that the program contained in this submission is completely\n",
        "   my own individual work, except where explicitly noted by comments that\n",
        "   provide details otherwise.  I understand that work that has been developed\n",
        "   by another student, or by me in collaboration with other students,\n",
        "   or by non-students as a result of request, solicitation, or payment,\n",
        "   may not be submitted for assessment in this subject.  I understand that\n",
        "   submitting for assessment work developed by or in collaboration with\n",
        "   other students or non-students constitutes Academic Misconduct, and\n",
        "   may be penalized by mark deductions, or by other penalties determined\n",
        "   via the University of Melbourne Academic Honesty Policy, as described\n",
        "   at https://academicintegrity.unimelb.edu.au.\n",
        "\n",
        "   (2) I also certify that I have not provided a copy of this work in either\n",
        "   softcopy or hardcopy or any other form to any other student, and nor will\n",
        "   I do so until after the marks are released. I understand that providing\n",
        "   my work to other students, regardless of my intention or any undertakings\n",
        "   made to me by that other student, is also Academic Misconduct.\n",
        "\n",
        "   (3) I further understand that providing a copy of the assignment\n",
        "   specification to any form of code authoring or assignment tutoring\n",
        "   service, or drawing the attention of others to such services and code\n",
        "   that may have been made available via such a service, may be regarded\n",
        "   as Student General Misconduct (interfering with the teaching activities\n",
        "   of the University and/or inciting others to commit Academic Misconduct).\n",
        "   I understand that an allegation of Student General Misconduct may arise\n",
        "   regardless of whether or not I personally make use of such solutions\n",
        "   or sought benefit from such actions.\n",
        "\n",
        "   <b>Signed by</b>: -\n",
        "   \n",
        "   <b>Dated</b>: 17/03/2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGolCsHMPtce"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}